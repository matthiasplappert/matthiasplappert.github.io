<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-81199330-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());

      gtag('config', 'UA-81199330-2');
    </script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Matthias Plappert</title>
    <meta name="description" content="Personal website of Matthias Plappert, a research scientist with an interest in machine learning and robotics.">

    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/font-awesome.min.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="container">
      <div class="row header">
        <div class="col-sm-4 avatar">
          <img src="images/avatar.jpg" alt="Matthias Plappert">
        </div>
        <div class="col-sm-8">
          <h1>Matthias Plappert</h1>
          <p>
            I am a research scientist with an interest in machine learning, robotics, reinforcement learning, and code synthesis.
          </p>
          <div class="contact">
          <ul>
            <li>
              <i class="fa fa-github" aria-hidden="true"></i> <a href="https://github.com/matthiasplappert">GitHub</a>
            </li>
            <li>
              <i class="fa fa-google" aria-hidden="true"></i> <a href="https://scholar.google.com/citations?user=MHQv5YUAAAAJ">Scholar</a>
            </li>
            <li>
              <i class="fa fa-envelope" aria-hidden="true"></i> <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#109;&#097;&#116;&#116;&#104;&#105;&#097;&#115;&#112;&#108;&#097;&#112;&#112;&#101;&#114;&#116;&#064;&#109;&#101;&#046;&#099;&#111;&#109;">Email</a>
            </li>
            <!-- <li>
              <i class="fa fa-file-pdf-o" aria-hidden="true"></i> <a href="cv.pdf">CV</a>
            </li> -->
          </ul>
          </div>
        </div>
      </div>
      <div class="row">
        <h2>Publications</h2>

        <h3>Journals</h3>
        <ul>
          <li>
            OpenAI: M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, <strong>M. Plappert</strong>, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba, <a href="https://journals.sagepub.com/doi/full/10.1177/0278364919887447">Learning Dexterous In-Hand Manipulation</a>, <em>The International Journal of Robotics Research (IJRR), Vol. 39(1), pp. 3-20</em>, January 2020 (pre-print August 2018) [<a href="publications/2020_OpenAI_Dexterous-Manipulation_IJRR.pdf">pdf</a>, <a href="https://arxiv.org/abs/1808.00177">pre-print</a>, <a href="https://blog.openai.com/learning-dexterity/">blog post</a>, <a href="https://www.youtube.com/watch?v=jwSbzNHGflM">video 1</a>, <a href="https://www.youtube.com/watch?v=DKe8FumoD4E">video 2</a>, <a href="publications/2020_OpenAI_Dexterous-Manipulation.bib">bib</a>]
          </li>
          <li>
            <strong>M. Plappert</strong>, C. Mandery, and T. Asfour, <a href="https://doi.org/10.1016/j.robot.2018.07.006">Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks</a>, <em>Robotics and Autonomous Systems, Vol. 109, pp. 13-26</em>, November 2018 [<a href="publications/2018_Plappert_Deep-Motion-Language-Mapping.pdf">pdf</a>, <a href="https://www.youtube.com/watch?v=2UQWOZtsg-8">video</a>, <a href="https://motion-annotation.humanoids.kit.edu/dataset">dataset</a>, <a href="https://gitlab.com/h2t/DeepMotionLanguageMapping">code</a>, <a href="publications/2018_Plappert_Deep-Motion-Language-Mapping.bib">bib</a>]
          </li>
          <li>
            <strong>M. Plappert</strong>, C. Mandery, and T. Asfour, <a href="https://doi.org/10.1089/big.2016.0028">The KIT Motion-Language Dataset</a>, <em>Big Data, Vol. 4, No. 4, pp. 236-252</em>, December 2016 [<a href="publications/2016_Plappert_Big-Data.pdf">pdf</a>, <a href="https://motion-annotation.humanoids.kit.edu/dataset">dataset</a>, <a href="https://github.com/matthiasplappert/motion-annotation-tool">code</a>, <a href="publications/2016_Plappert_Big-Data.bib">bib</a>]
          </li>
        </ul>

        <h3>Conferences</h3>
        <ul>
          <li>
            <strong>M. Plappert</strong>, R. Houthooft, P. Dhariwal, S. Sidor, R.Y. Chen, X. Chen, T. Asfour, P. Abbeel, and M. Andrychowicz, <a href="https://openreview.net/forum?id=ByBAl2eAZ">Parameter Space Noise for Exploration</a>, <em>In the proceedings of the International Conference on Learning Representations (ICLR), Vancouver, Canada</em>, April 2018 [<a href="publications/2018_Plappert_Param-Noise-ICLR.pdf">pdf</a>, <a href="https://blog.openai.com/better-exploration-with-parameter-noise/">blog post</a>, <a href="https://github.com/openai/baselines">code</a>, <a href="publications/2018_Plappert_Param-Noise-ICLR.bib">bib</a>]
          </li>
          <li>
            C. Mandery, <strong>M. Plappert</strong>, J. Borr√†s, and T. Asfour, <a href="http://ieeexplore.ieee.org/document/7527910/">Dimensionality Reduction for Whole-Body Human Motion Recognition</a>, <em>19th International Conference on Information Fusion (FUSION), pp. 355-362</em>, July 2016 [<a href="publications/2016_Mandery_FUSION.pdf">pdf</a>, <a href="publications/2016_Mandery_FUSION.bib">bib</a>]
          </li>
        </ul>

        <h3>Pre-Prints & Tech Reports</h3>
        <ul>
          <li>
            M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. Such, D. Cummings, <strong>M. Plappert</strong>, F. Chantzis, E. Barnes, A. Herbert-Voss, W.H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A.N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, <a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a>, <em>2107.03374</em>, July 2021 [<a
              href="publications/2021_OpenAI_Codex.pdf">pdf</a>, <a href="https://openai.com/blog/openai-codex/">blog post</a>]
          </li>

          <li>
            OpenAI: <strong>M. Plappert</strong>, R. Sampedro, T. Xu, I. Akkaya, V. Kosaraju, P. Welinder, R. D'Sa,
            A. Petron, H. Ponde de Oliveira Pinto, A. Paino, H. Noh, L. Weng, Q. Yuan, C. Chu, and W.
            Zaremba, <a href="https://arxiv.org/abs/2101.04882">Asymmetric self-play for automatic goal discovery in robotic manipulation</a>, <em>arXiv:2101.04882</em>, January 2021 [<a
              href="publications/2021_OpenAI_Asymmetric-self-play.pdf">pdf</a>, <a href="https://robotics-self-play.github.io">videos</a>]
          </li>
          <li>
            L. Zhang, <strong>M. Plappert</strong>, and W. Zaremba, <a href="https://arxiv.org/abs/2009.12864">Predicting Sim-to-Real Transfer with Probabilistic Dynamics Models</a>, <em>arXiv:2009.12864</em>, September 2020 [<a href="publications/2020_Zhang_Transfer.pdf">pdf</a>]
          </li>
          <li>
            OpenAI: I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, <strong>M. Plappert</strong>, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang, <a href="https://arxiv.org/abs/1910.07113">Solving Rubik's Cube with a Robot Hand</a>, <em>arXiv:1910.07113</em>, October 2019 [<a href="publications/2019_OpenAI_Rubiks-Cube.pdf">pdf</a>, <a href="https://openai.com/blog/solving-rubiks-cube/">blog post</a>, <a href="https://www.youtube.com/playlist?list=PLOXw6I10VTv9HODt7TFEL72K3Q6C4itG6">all videos</a>, <a href="publications/2019_OpenAI_Rubiks-Cube.bib">bib</a>]
          </li>
          <li>
            <strong>M. Plappert</strong>, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Powell, J. Schneider, J. Tobin, M. Chociej, P. Welinder, V. Kumar, and W. Zaremba, <a href="https://arxiv.org/abs/1802.09464">Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research</a>, <em>arXiv:1802.09464</em>, February 2018 [<a href="publications/2018_Plappert_MultiGoalRL.pdf">pdf</a>, <a href="https://blog.openai.com/ingredients-for-robotics-research/">blog post</a>, <a href="https://github.com/openai/gym/tree/master/gym/envs/robotics">code</a>, <a href="publications/2018_Plappert_MultiGoalRL.bib">bib</a>]
          </li>
        </ul>

        <h3>Theses</h3>
        <ul>
          <li>
            <strong>M. Plappert</strong>, Parameter Space Noise for Exploration in Deep Reinforcement Learning, <em>Master thesis</em>, November 2017 [<a href="publications/2017_Plappert_Master-thesis.pdf">pdf</a>, <a href="publications/2017_Plappert_Master-thesis.bib">bib</a>]
          </li>
          <li>
            <strong>M. Plappert</strong>, <a href="http://services.bibliothek.kit.edu/primo/start.php?recordid=KITSRC44651960X">Classification of Human Whole-Body Motion using Hidden Markov Models</a>, <em>Bachelor thesis</em>, August 2015 [<a href="publications/2015_Plappert_Bachelor-thesis.pdf">pdf</a>, <a href="https://github.com/matthiasplappert/motion-classification">code</a>, <a href="publications/2015_Plappert_Bachelor-thesis.bib">bib</a>]
          </li>
        </ul>
      </div>

      <div class="row">
        <h2>Talks</h2>
        <ul>
          <li>
            <a href="https://openai.com/blog/symposium-2019/">OpenAI Robotics Symposium 2019</a>, April 2019, [<a href="https://www.youtube.com/watch?v=WRsxoVB8Yng&feature=youtu.be&t=3442">video</a>]
          </li>
          <li>
            <a href="https://sites.google.com/view/deep-rl-workshop-nips-2018/home">Learning Dexterity</a>, <em>NeurIPS 2018, Deep Reinforcement Learning Workshop</em>, December 2018 [<a href="talks/2018-12-neurips-learning-dexterity-slides.pdf">slides</a>, <a href="talks/2018-12-neurips-learning-dexterity-poster.pdf">poster</a>]
          </li>
          <li>
            <a href="https://www.meetup.com/de-DE/karlsruhe-ai/events/254141416/">Learning Dexterity</a>, <em>karlsruhe.ai / Hack &amp; S√∂hne</em>, September 2018 [<a href="talks/2018-09-06-learning-dexterity.pdf">slides</a>]
          </li>
          <li>
            <a href="https://www.meetup.com/Heidelberg-Artificial-Intelligence-Meetup/events/249605800/">Parameter Space Noise for Exploration</a>, <em>heidelberg.ai</em>, May 2018 [<a href="talks/2018-05-15-param-noise.pdf">slides</a>]
          </li>
        </ul>
      </div>

      <div class="row">
        <h2>Projects</h2>
        <ul>
          <li>
            <a href="https://github.com/keras-rl/keras-rl">Keras-RL</a>, a deep reinforcement learning library for Keras [<a href="http://keras-rl.readthedocs.io/en/latest/">docs</a>]
          </li>
          <li>
            <a href="https://github.com/matthiasplappert/pibot">PiBot</a>, a simple robot platform based on a Raspberry Pi
          </li>
          <li>
            <a href="https://github.com/matthiasplappert/motion-annotation-tool">Motion Annotation Tool</a>, a web-based tool to collect natural language descriptions of human whole-body motion [<a href="https://motion-annotation.humanoids.kit.edu">website</a>]
          </li>
        </ul>
      </div>

      <div class="row">
        <h2>Coverage</h2>
        <ul>
          <li>
            Codex and GitHub Copilot:
            <ul class="comma-separated">
              <li><a href="https://www.nytimes.com/2021/09/09/technology/codex-artificial-intelligence-coding.html"><strong>New York Times</strong></a></li>
              <li><a href="https://www.wired.com/story/plaintext-open-ai-codex/"><strong>WIRED</strong></a></li>
              <li><a href="https://en.wikipedia.org/wiki/OpenAI_Codex"><strong>Wikipedia</strong></a></li>
              <li><a href="https://www.cnbc.com/2021/06/29/microsoft-github-copilot-ai-offers-coding-suggestions.html">CNBC</a></li>
              <li><a href="https://www.theverge.com/2021/8/10/22618128/openai-codex-natural-language-into-code-api-beta-access">The Verge</a></li>
              <li><a href="https://techcrunch.com/2021/08/10/openai-upgrades-its-natural-language-ai-coder-codex-and-kicks-off-private-beta/">TechCrunch</a></li>
              <li><a href="https://venturebeat.com/2021/08/10/openai-launches-codex-an-api-for-translating-natural-language-into-code/">VentureBeat</a></li>
              <li><a href="https://www.theregister.com/2021/08/16/in_brief_ai/">The Register</a></li>
            </ul>
          </li>
          <li>
            Solving Rubik's Cube with a Robot Hand:
            <ul class="comma-separated">
              <li><a href="https://www.nytimes.com/2019/10/15/technology/if-a-robotic-hand-solves-a-rubiks-cube-does-it-prove-something.html"><strong>New York Times</strong></a></li>
              <li><a href="https://www.bbc.com/news/technology-50064225"><strong>BBC</strong></a></li>
              <li><a href="https://www.vox.com/future-perfect/2019/10/15/20910007/robot-rubiks-cube-one-handed"><strong>Vox</strong></a></li>
              <li><a href="https://www.technologyreview.com/s/614554/a-robot-hand-taught-itself-to-solve-a-rubiks-cube-after-creating-its-own-training-regime/"><strong>MIT Technology Review</strong></a></li>
              <li><a href="https://spectrum.ieee.org/automaton/robotics/robotics-hardware/openai-demonstrates-sim2real-by-with-onehanded-rubiks-cube-solving"><strong>IEEE Spectrum</strong></a></li>
              <li><a href="https://www.washingtonpost.com/technology/2019/10/18/this-robotic-hand-learned-solve-rubiks-cube-its-own-just-like-human/">Washington Post</a></li>
              <li><a href="https://nypost.com/2019/10/16/robot-hand-teaches-itself-how-to-solve-a-rubiks-cube/">New York Post</a></li>
              <li><a href="https://www.vice.com/en_us/article/7x5dye/watch-this-humanoid-robot-hand-solve-a-rubiks-cube">Vice</a></li>
              <li><a href="https://www.popularmechanics.com/technology/robots/amp29485996/rubiks-cube-robot/">Popular Mechanics</a></li>
              <li><a href="https://syncedreview.com/2019/10/15/openai-robot-hand-today-rubiks-cube-tomorrow-the-real-world/">Synced</a></li>
              <li><a href="https://fortune.com/2019/10/15/why-most-companies-are-failing-at-artificial-intelligence-eye-on-a-i/">Fortune</a></li>
              <li><a href="https://www.zdnet.com/article/slight-of-hand-openais-trick-to-make-its-rubiks-robot-hand-work/">ZDNet</a></li>
              <li><a href="https://www.theverge.com/2019/10/15/20914575/openai-dactyl-robotic-hand-rubiks-cube-one-handed-solve-dexterity-ai">The Verge</a></li>
              <li><a href="https://venturebeat.com/2019/10/15/openai-teaches-a-robotic-hand-to-solve-a-rubiks-cube/">VentureBeat</a></li>
              <li><a href="https://techcrunch.com/2019/10/15/watch-openais-human-like-robot-solve-a-rubiks-cube-one-handed/">TechCrunch</a></li>
              <li><a href="https://www.newscientist.com/article/2219939-extremely-dexterous-robot-can-solve-a-rubiks-cube-one-handed/">New Scientist</a></li>
            </ul>
          </li>
          <li>
            Learning Dexterous In-Hand Manipulation:
            <ul class="comma-separated">
              <li><strong><a href="https://www.nytimes.com/interactive/2018/07/30/technology/robot-hands.html">New York Times</a></strong></li>
              <li><strong><a href="https://www.wired.com/story/this-robot-hand-taught-itself-how-to-grab-stuff-like-a-human/">WIRED</a></strong></li>
              <li><strong><a href="https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/openai-demonstrates-complex-manipulation-transfer-from-simulation-to-real-world">IEEE Spectrum</a></strong></li>
              <li><strong><a href="http://www.sciencemag.org/news/2018/07/watch-robot-hand-learn-manipulate-objects-just-human-hand">Science</a></strong></li>
              <li><a href="https://www.technologyreview.com/s/611724/artificial-intelligence-driven-robot-hand-spends-a-hundred-years-teaching-itself-to-rotate/">MIT Technology Review</a></li>
              <li><a href="https://www.telegraph.co.uk/technology/2018/07/31/robot-breakthrough-elon-musks-ai-group-creates-human-like-hand/">The Telegraph</a></li>
              <li><a href="https://www.bloomberg.com/news/articles/2018-07-31/a-robotic-hand-can-juggle-a-cube-with-lots-of-training">Bloomberg</a></li>
              <li><a href="https://www.sfchronicle.com/business/technology/article/A-robotic-hand-can-juggle-a-cube-with-lots-of-13119510.php">San Francisco Chronicle</a></li>
              <li><a href="https://www.theregister.co.uk/2018/07/30/openai_robot_hand/">The Register</a></li>
              <li><a href="https://apnews.com/64952b7ea20342fc9ccaceb85bbb2f61">Associated Press</a></li>
              <li><a href="https://uk.reuters.com/article/us-robotics-openai/robot-hand-learns-real-world-moves-in-virtual-training-idUKKBN1KK1XG">Reuters</a></li>
              <li><a href="https://www.theverge.com/2018/7/30/17621112/openai-robot-dexterity-dactyl-artificial-intelligence">The Verge</a></li>
              <li><a href="https://www.axios.com/robot-hand-is-breakthrough-for-more-capable-ai-f2745640-8cc4-4941-96ac-2472da52b536.html">Axios</a></li>
              <li><a href="https://techcrunch.com/2018/07/30/openais-robotic-hand-doesnt-need-humans-to-teach-it-human-behaviors/">TechCrunch</a></li>
              <li><a href="https://www.engadget.com/2018/07/30/openai-dactyl-improves-dexterity-robot-hands/">Engadget</a></li>
              <li><a href="https://futurism.com/openai-general-purpose-algorithms/">Futurism</a></li>
              <li><a href="https://www.golem.de/news/openai-roboterhand-erhaelt-feinmotorik-dank-ki-1807-135767.html">golem.de</a></li>
              <li><a href="https://www.spektrum.de/news/binnen-hundert-jahren-gelernt/1582772">Spektrum.de</a></li>
            </ul>
          </li>
          <li>
            Ingredients for Robotics Research:
            <ul class="comma-separated">
              <li><strong><a href="https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/openai-releases-algorithm-that-helps-robots-learn-from-hindsight">IEEE Spectrum</a></strong></li>
              <li><strong><a href="https://www.technologyreview.com/the-download/610394/theres-a-new-way-to-have-robots-learn-from-their-mistakes/">MIT Technology Review</a></strong></li>
              <li><a href="https://www.theregister.co.uk/2018/02/26/openai_gym_robotics/">The Register</a></li>
              <li><a href="http://www.newsweek.com/algorithm-lets-ai-learn-mistakes-830358">Newsweek</a></li>
              <li><a href="https://futurism.com/ai-learn-mistakes-openai">Futurism</a></li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
  </body>
</html>
